{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.poincare import PoincareModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspaces/master_thesis/mapping/data_ready_to_use.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_id</th>\n",
       "      <th>concept_name</th>\n",
       "      <th>concept_synonym_name</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>preprocessed_synonyms</th>\n",
       "      <th>preprocessed_without_stemming</th>\n",
       "      <th>preprocessed_synonyms_without_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4001098</td>\n",
       "      <td>Radiating chest pain</td>\n",
       "      <td>Radiating chest pain (finding)</td>\n",
       "      <td>radiat chest pain</td>\n",
       "      <td>radiat chest pain find</td>\n",
       "      <td>radiating chest pain</td>\n",
       "      <td>radiating chest pain finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37392117</td>\n",
       "      <td>Urine tryptophan:creatinine ratio</td>\n",
       "      <td>Urine tryptophan:creatinine ratio (observable ...</td>\n",
       "      <td>urin tryptophan creatinin ratio</td>\n",
       "      <td>urin tryptophan creatinin ratio observ entiti</td>\n",
       "      <td>urine tryptophan creatinine ratio</td>\n",
       "      <td>urine tryptophan creatinine ratio observable e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37398455</td>\n",
       "      <td>Urine threonine:creatinine ratio</td>\n",
       "      <td>Urine threonine:creatinine ratio (observable e...</td>\n",
       "      <td>urin threonin creatinin ratio</td>\n",
       "      <td>urin threonin creatinin ratio observ entiti</td>\n",
       "      <td>urine threonine creatinine ratio</td>\n",
       "      <td>urine threonine creatinine ratio observable en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37392118</td>\n",
       "      <td>Urine taurine:creatinine ratio</td>\n",
       "      <td>Urine taurine:creatinine ratio (observable ent...</td>\n",
       "      <td>urin taurin creatinin ratio</td>\n",
       "      <td>urin taurin creatinin ratio observ entiti</td>\n",
       "      <td>urine taurine creatinine ratio</td>\n",
       "      <td>urine taurine creatinine ratio observable entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37392119</td>\n",
       "      <td>Urine phenylalanine:creatinine ratio</td>\n",
       "      <td>Urine phenylalanine:creatinine ratio (observab...</td>\n",
       "      <td>urin phenylalanin creatinin ratio</td>\n",
       "      <td>urin phenylalanin creatinin ratio observ entiti</td>\n",
       "      <td>urine phenylalanine creatinine ratio</td>\n",
       "      <td>urine phenylalanine creatinine ratio observabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491491</th>\n",
       "      <td>37398450</td>\n",
       "      <td>Urine homocysteine:creatinine ratio</td>\n",
       "      <td>Urine homocysteine:creatinine ratio (observabl...</td>\n",
       "      <td>urin homocystein creatinin ratio</td>\n",
       "      <td>urin homocystein creatinin ratio observ entiti</td>\n",
       "      <td>urine homocysteine creatinine ratio</td>\n",
       "      <td>urine homocysteine creatinine ratio observable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491492</th>\n",
       "      <td>37398451</td>\n",
       "      <td>Urine aspartate:creatinine ratio</td>\n",
       "      <td>Urine aspartate:creatinine ratio (observable e...</td>\n",
       "      <td>urin aspart creatinin ratio</td>\n",
       "      <td>urin aspart creatinin ratio observ entiti</td>\n",
       "      <td>urine aspartate creatinine ratio</td>\n",
       "      <td>urine aspartate creatinine ratio observable en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491493</th>\n",
       "      <td>37398452</td>\n",
       "      <td>Urine alanine:creatinine ratio</td>\n",
       "      <td>Urine alanine:creatinine ratio (observable ent...</td>\n",
       "      <td>urin alanin creatinin ratio</td>\n",
       "      <td>urin alanin creatinin ratio observ entiti</td>\n",
       "      <td>urine alanine creatinine ratio</td>\n",
       "      <td>urine alanine creatinine ratio observable entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491494</th>\n",
       "      <td>37398453</td>\n",
       "      <td>Urine valine:creatinine ratio</td>\n",
       "      <td>Urine valine:creatinine ratio (observable entity)</td>\n",
       "      <td>urin valin creatinin ratio</td>\n",
       "      <td>urin valin creatinin ratio observ entiti</td>\n",
       "      <td>urine valine creatinine ratio</td>\n",
       "      <td>urine valine creatinine ratio observable entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491495</th>\n",
       "      <td>37398454</td>\n",
       "      <td>Urine tyrosine:creatinine ratio</td>\n",
       "      <td>Urine tyrosine:creatinine ratio (observable en...</td>\n",
       "      <td>urin tyrosin creatinin ratio</td>\n",
       "      <td>urin tyrosin creatinin ratio observ entiti</td>\n",
       "      <td>urine tyrosine creatinine ratio</td>\n",
       "      <td>urine tyrosine creatinine ratio observable entity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491496 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        concept_id                          concept_name   \n",
       "0          4001098                  Radiating chest pain  \\\n",
       "1         37392117     Urine tryptophan:creatinine ratio   \n",
       "2         37398455      Urine threonine:creatinine ratio   \n",
       "3         37392118        Urine taurine:creatinine ratio   \n",
       "4         37392119  Urine phenylalanine:creatinine ratio   \n",
       "...            ...                                   ...   \n",
       "491491    37398450   Urine homocysteine:creatinine ratio   \n",
       "491492    37398451      Urine aspartate:creatinine ratio   \n",
       "491493    37398452        Urine alanine:creatinine ratio   \n",
       "491494    37398453         Urine valine:creatinine ratio   \n",
       "491495    37398454       Urine tyrosine:creatinine ratio   \n",
       "\n",
       "                                     concept_synonym_name   \n",
       "0                          Radiating chest pain (finding)  \\\n",
       "1       Urine tryptophan:creatinine ratio (observable ...   \n",
       "2       Urine threonine:creatinine ratio (observable e...   \n",
       "3       Urine taurine:creatinine ratio (observable ent...   \n",
       "4       Urine phenylalanine:creatinine ratio (observab...   \n",
       "...                                                   ...   \n",
       "491491  Urine homocysteine:creatinine ratio (observabl...   \n",
       "491492  Urine aspartate:creatinine ratio (observable e...   \n",
       "491493  Urine alanine:creatinine ratio (observable ent...   \n",
       "491494  Urine valine:creatinine ratio (observable entity)   \n",
       "491495  Urine tyrosine:creatinine ratio (observable en...   \n",
       "\n",
       "                             preprocessed   \n",
       "0                       radiat chest pain  \\\n",
       "1         urin tryptophan creatinin ratio   \n",
       "2           urin threonin creatinin ratio   \n",
       "3             urin taurin creatinin ratio   \n",
       "4       urin phenylalanin creatinin ratio   \n",
       "...                                   ...   \n",
       "491491   urin homocystein creatinin ratio   \n",
       "491492        urin aspart creatinin ratio   \n",
       "491493        urin alanin creatinin ratio   \n",
       "491494         urin valin creatinin ratio   \n",
       "491495       urin tyrosin creatinin ratio   \n",
       "\n",
       "                                  preprocessed_synonyms   \n",
       "0                                radiat chest pain find  \\\n",
       "1         urin tryptophan creatinin ratio observ entiti   \n",
       "2           urin threonin creatinin ratio observ entiti   \n",
       "3             urin taurin creatinin ratio observ entiti   \n",
       "4       urin phenylalanin creatinin ratio observ entiti   \n",
       "...                                                 ...   \n",
       "491491   urin homocystein creatinin ratio observ entiti   \n",
       "491492        urin aspart creatinin ratio observ entiti   \n",
       "491493        urin alanin creatinin ratio observ entiti   \n",
       "491494         urin valin creatinin ratio observ entiti   \n",
       "491495       urin tyrosin creatinin ratio observ entiti   \n",
       "\n",
       "               preprocessed_without_stemming   \n",
       "0                       radiating chest pain  \\\n",
       "1          urine tryptophan creatinine ratio   \n",
       "2           urine threonine creatinine ratio   \n",
       "3             urine taurine creatinine ratio   \n",
       "4       urine phenylalanine creatinine ratio   \n",
       "...                                      ...   \n",
       "491491   urine homocysteine creatinine ratio   \n",
       "491492      urine aspartate creatinine ratio   \n",
       "491493        urine alanine creatinine ratio   \n",
       "491494         urine valine creatinine ratio   \n",
       "491495       urine tyrosine creatinine ratio   \n",
       "\n",
       "                   preprocessed_synonyms_without_stemming  \n",
       "0                            radiating chest pain finding  \n",
       "1       urine tryptophan creatinine ratio observable e...  \n",
       "2       urine threonine creatinine ratio observable en...  \n",
       "3        urine taurine creatinine ratio observable entity  \n",
       "4       urine phenylalanine creatinine ratio observabl...  \n",
       "...                                                   ...  \n",
       "491491  urine homocysteine creatinine ratio observable...  \n",
       "491492  urine aspartate creatinine ratio observable en...  \n",
       "491493   urine alanine creatinine ratio observable entity  \n",
       "491494    urine valine creatinine ratio observable entity  \n",
       "491495  urine tyrosine creatinine ratio observable entity  \n",
       "\n",
       "[491496 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseEmbeddingDataset(Dataset):\n",
    "    def __init__(self, X, y, w2v_model, poincare_model, max_len=20):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w2v_model = w2v_model\n",
    "        self.poincare_model = poincare_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get Word2Vec embedding\n",
    "        X = self.get_phrase_vector(self.X.iloc[idx], self.w2v_model, self.max_len)\n",
    "        \n",
    "        # Get Poincare embedding\n",
    "        y = torch.tensor(self.poincare_model.kv[self.y.iloc[idx]], dtype=torch.float)\n",
    "        #y = torch.tensor(self.poincare_model.wv[str(self.y.iloc[idx])], dtype=torch.float)\n",
    "        return X, y\n",
    "\n",
    "    @staticmethod\n",
    "    def get_phrase_vector(phrase, model, max_len):\n",
    "        words = str(phrase).split()\n",
    "        phrase_vector = np.zeros((max_len, model.vector_size))\n",
    "\n",
    "        for i in range(max_len):\n",
    "            if i < len(words) and words[i] in model.wv:\n",
    "                phrase_vector[i] = model.wv[words[i]]\n",
    "\n",
    "        phrase_vector = phrase_vector.flatten()\n",
    "        \n",
    "        return torch.tensor(phrase_vector, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"/workspaces/master_thesis/word2vec_pubmed.model\")\n",
    "poincare_model = PoincareModel.load('/workspaces/master_thesis/poincare_100d_concept_id')\n",
    "#deepwalk_model = Word2Vec.load(\"/workspaces/master_thesis/deepwalk_snomed.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your phrases into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_synonyms_without_stemming'], df['concept_id'], test_size=0.02, random_state=42)\n",
    "\n",
    "# Create your datasets\n",
    "train_dataset = PhraseEmbeddingDataset(X_train, y_train, w2v_model, poincare_model)\n",
    "#train_dataset = PhraseEmbeddingDataset(X_train, y_train, w2v_model, deepwalk_model)\n",
    "test_dataset = PhraseEmbeddingDataset(X_test, y_test, w2v_model, poincare_model)\n",
    "#test_dataset = PhraseEmbeddingDataset(X_test, y_test, w2v_model, deepwalk_model)\n",
    "\n",
    "# Create your data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nhead=10, num_layers=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=input_size, nhead=nhead, dim_feedforward=hidden_size)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(input_size, output_size)  # Adjusting the input dimension of the FC layer to match the output of the TransformerEncoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input to (seq_len, batch_size, features)\n",
    "        x = x.view(20, x.size(0), 300)  # TransformerEncoder expects (seq_len, batch_size, features)\n",
    "        # Forward propagate transformer\n",
    "        out = self.transformer(x)  # out: tensor of shape (seq_len, batch_size, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[-1])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/7527], Loss: 0.0167\n",
      "Epoch [1/10], Step [200/7527], Loss: 0.0102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[39m=\u001b[39m model(phrases)\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m20\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m300\u001b[39m)  \u001b[39m# TransformerEncoder expects (seq_len, batch_size, features)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Forward propagate transformer\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)  \u001b[39m# out: tensor of shape (seq_len, batch_size, hidden_size)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Decode the hidden state of the last time step\u001b[39;00m\n\u001b[1;32m     19\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:592\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39mis_causal))\n\u001b[0;32m--> 592\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ff_block(x))\n\u001b[1;32m    594\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:607\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ff_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 607\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x))))\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(300, 300, 100).to(device)\n",
    "criterion = nn.MSELoss()  # adjust the loss function to your problem\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (phrases, labels) in enumerate(train_loader):\n",
    "        phrases = phrases.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(phrases)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
