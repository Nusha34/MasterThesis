{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models.poincare import PoincareModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"/workspaces/master_thesis/word2vec_wiki_snomed_preprocessed.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_model=PoincareModel.load('/workspaces/master_thesis/poincare_100d_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is not strictly necessary, but it can be helpful when combining embeddings from different models. Normalizing the embeddings ensures that they are on the same scale, which can lead to more meaningful results when combining them using methods like weighted sum or concatenation.\n",
    "\n",
    "If the embeddings from the two models are on very different scales or if one model's embeddings dominate the other's, the combined embeddings might be heavily influenced by one model, which can lead to a loss of valuable information from the other model. Normalizing the embeddings mitigates this issue by ensuring that both models contribute more evenly to the combined embeddings.\n",
    "\n",
    "If you decide not to normalize the embeddings, you can still combine them using the weighted sum or concatenation techniques. However, you might need to carefully adjust the weights in the weighted sum method to balance the contributions from both models. Alternatively, you can experiment with and without normalization and compare the results to see which approach works better for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_poincare_embeddings = normalize_embeddings(poincare_model.kv.vectors)\n",
    "normalized_word2vec_embeddings = normalize_embeddings(word2vec_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_weighted_sum(embedding1, embedding2, weight1, weight2):\n",
    "    return weight1 * embedding1 + weight2 * embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, word2vec_model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        return None\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/workspaces/master_thesis/snomed_preprocessed\", \"rb\") as fp:   # Unpickling\n",
    "  concepts = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_concepts = [' '.join(concept) for concept in concepts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_word2vec_embeddings = {}\n",
    "\n",
    "for concept in list_of_concepts:\n",
    "    concept_word2vec_embeddings[concept] = sentence_embedding(concept, word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1263358,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_794946/3724771994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalized_word2vec_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcept_word2vec_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_794946/3876937059.py\u001b[0m in \u001b[0;36mnormalize_embeddings\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m     \"\"\"\n\u001b[0;32m-> 2493\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1263358,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "normalized_word2vec_embeddings = normalize_embeddings(list(concept_word2vec_embeddings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_embeddings = {}\n",
    "weight1 = 0.5  \n",
    "weight2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (300,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_794946/4128585432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconcept\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_concepts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconcept\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoincare_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconcept\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcept_word2vec_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         combined_embeddings[concept] = combine_weighted_sum(\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mpoincare_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mconcept_word2vec_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_794946/2179083699.py\u001b[0m in \u001b[0;36mcombine_weighted_sum\u001b[0;34m(embedding1, embedding2, weight1, weight2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_weighted_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mweight1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membedding1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membedding2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (300,) "
     ]
    }
   ],
   "source": [
    "for concept in list_of_concepts:\n",
    "    if concept in poincare_model.kv and concept in concept_word2vec_embeddings:\n",
    "        combined_embeddings[concept] = combine_weighted_sum(\n",
    "            normalized_poincare_embeddings[poincare_model.kv.key_to_index[concept]],\n",
    "            normalized_word2vec_embeddings[list(concept_word2vec_embeddings.keys()).index(concept)],\n",
    "            weight1, weight2\n",
    "        )\n",
    "    elif concept in concept_word2vec_embeddings:\n",
    "        combined_embeddings[concept] = concept_word2vec_embeddings[concept]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
